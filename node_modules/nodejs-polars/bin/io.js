"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.readJSONStream = exports.readCSVStream = exports.scanIPC = exports.readIPC = exports.scanParquet = exports.readAvro = exports.readParquet = exports.scanJson = exports.readJSON = exports.scanCSV = exports.readCSV = exports.readRecords = void 0;
const polars_internal_1 = __importDefault(require("./internals/polars_internal"));
const dataframe_1 = require("./dataframe");
const utils_1 = require("./utils");
const dataframe_2 = require("./lazy/dataframe");
const stream_1 = require("stream");
const functions_1 = require("./functions");
const readCsvDefaultOptions = {
    inferSchemaLength: 100,
    hasHeader: true,
    ignoreErrors: true,
    chunkSize: 10000,
    skipRows: 0,
    sep: ",",
    rechunk: false,
    encoding: "utf8",
    lowMemory: false,
    parseDates: false,
    skipRowsAfterHeader: 0,
};
const readJsonDefaultOptions = {
    batchSize: 10000,
    inferSchemaLength: 50,
    format: "json",
};
// utility to read streams as lines.
class LineBatcher extends stream_1.Stream.Transform {
    #lines;
    #accumulatedLines;
    #batchSize;
    constructor(options) {
        super(options);
        this.#lines = [];
        this.#accumulatedLines = 0;
        this.#batchSize = options.batchSize;
    }
    _transform(chunk, _encoding, done) {
        let begin = 0;
        let i = 0;
        while (i < chunk.length) {
            if (chunk[i] === 10) {
                // '\n'
                this.#accumulatedLines++;
                if (this.#accumulatedLines === this.#batchSize) {
                    this.#lines.push(chunk.subarray(begin, i + 1));
                    this.push(Buffer.concat(this.#lines));
                    this.#lines = [];
                    this.#accumulatedLines = 0;
                    begin = i + 1;
                }
            }
            i++;
        }
        this.#lines.push(chunk.subarray(begin));
        done();
    }
    _flush(done) {
        this.push(Buffer.concat(this.#lines));
        done();
    }
}
// helper functions
function readCSVBuffer(buff, options) {
    return (0, dataframe_1._DataFrame)(polars_internal_1.default.readCsv(buff, { ...readCsvDefaultOptions, ...options }));
}
function readRecords(records, options) {
    if (options?.schema) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.fromRows(records, options.schema));
    }
    else {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.fromRows(records, undefined, options?.inferSchemaLength));
    }
}
exports.readRecords = readRecords;
function readCSV(pathOrBody, options) {
    options = { ...readCsvDefaultOptions, ...options };
    const extensions = [".tsv", ".csv"];
    if (Buffer.isBuffer(pathOrBody)) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.readCsv(pathOrBody, options));
    }
    if (typeof pathOrBody === "string") {
        const inline = !(0, utils_1.isPath)(pathOrBody, extensions);
        if (inline) {
            const buf = Buffer.from(pathOrBody, "utf-8");
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readCsv(buf, options));
        }
        else {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readCsv(pathOrBody, options));
        }
    }
    else {
        throw new Error("must supply either a path or body");
    }
}
exports.readCSV = readCSV;
const scanCsvDefaultOptions = {
    inferSchemaLength: 100,
    cache: true,
    hasHeader: true,
    ignoreErrors: true,
    skipRows: 0,
    sep: ",",
    rechunk: false,
    encoding: "utf8",
    lowMemory: false,
    parseDates: false,
    skipRowsAfterHeader: 0,
};
function scanCSV(path, options) {
    options = { ...scanCsvDefaultOptions, ...options };
    return (0, dataframe_2._LazyDataFrame)(polars_internal_1.default.scanCsv(path, options));
}
exports.scanCSV = scanCSV;
function readJSON(pathOrBody, options = readJsonDefaultOptions) {
    options = { ...readJsonDefaultOptions, ...options };
    const method = options.format === "lines" ? polars_internal_1.default.readJsonLines : polars_internal_1.default.readJson;
    const extensions = [".ndjson", ".json", ".jsonl"];
    if (Buffer.isBuffer(pathOrBody)) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.readJson(pathOrBody, options));
    }
    if (typeof pathOrBody === "string") {
        const inline = !(0, utils_1.isPath)(pathOrBody, extensions);
        if (inline) {
            return (0, dataframe_1._DataFrame)(method(Buffer.from(pathOrBody, "utf-8"), options));
        }
        else {
            return (0, dataframe_1._DataFrame)(method(pathOrBody, options));
        }
    }
    else {
        throw new Error("must supply either a path or body");
    }
}
exports.readJSON = readJSON;
function scanJson(path, options) {
    options = { ...readJsonDefaultOptions, ...options };
    return (0, dataframe_2._LazyDataFrame)(polars_internal_1.default.scanJson(path, options));
}
exports.scanJson = scanJson;
/**
   * Read into a DataFrame from a parquet file.
   * @param pathOrBuffer
   * Path to a file, list of files, or a file like object. If the path is a directory, that directory will be used
   * as partition aware scan.
   * @param options.columns Columns to select. Accepts a list of column indices (starting at zero) or a list of column names.
   * @param options.numRows  Stop reading from parquet file after reading ``numRows``.
   * @param options.parallel
   *    Any of  'auto' | 'columns' |  'row_groups' | 'none'
        This determines the direction of parallelism. 'auto' will try to determine the optimal direction.
        Defaults to 'auto'
   * @param options.rowCount Add row count as column
   */
function readParquet(pathOrBody, options) {
    const pliOptions = {};
    if (typeof options?.columns?.[0] === "number") {
        pliOptions.projection = options?.columns;
    }
    else {
        pliOptions.columns = options?.columns;
    }
    pliOptions.nRows = options?.numRows;
    pliOptions.rowCount = options?.rowCount;
    const parallel = options?.parallel ?? "auto";
    if (Buffer.isBuffer(pathOrBody)) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.readParquet(pathOrBody, pliOptions, parallel));
    }
    if (typeof pathOrBody === "string") {
        const inline = !(0, utils_1.isPath)(pathOrBody, [".parquet"]);
        if (inline) {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readParquet(Buffer.from(pathOrBody), pliOptions, parallel));
        }
        else {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readParquet(pathOrBody, pliOptions, parallel));
        }
    }
    else {
        throw new Error("must supply either a path or body");
    }
}
exports.readParquet = readParquet;
function readAvro(pathOrBody, options = {}) {
    if (Buffer.isBuffer(pathOrBody)) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.readAvro(pathOrBody, options));
    }
    if (typeof pathOrBody === "string") {
        const inline = !(0, utils_1.isPath)(pathOrBody, [".avro"]);
        if (inline) {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readAvro(Buffer.from(pathOrBody), options));
        }
        else {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readAvro(pathOrBody, options));
        }
    }
    else {
        throw new Error("must supply either a path or body");
    }
}
exports.readAvro = readAvro;
/**
 * __Lazily read from a parquet file or multiple files via glob patterns.__
 * ___
 * This allows the query optimizer to push down predicates and projections to the scan level,
 * thereby potentially reducing memory overhead.
 * @param path Path to a file or or glob pattern
 * @param options.numRows Stop reading from parquet file after reading ``numRows``.
 * @param options.cache Cache the result after reading.
 * @param options.parallel Read the parquet file in parallel. The single threaded reader consumes less memory.
 * @param options.rechunk In case of reading multiple files via a glob pattern rechunk the final DataFrame into contiguous memory chunks.
 */
function scanParquet(path, options = {}) {
    const pliOptions = {};
    pliOptions.nRows = options?.numRows;
    pliOptions.rowCount = options?.rowCount;
    pliOptions.parallel = options?.parallel ?? "auto";
    return (0, dataframe_2._LazyDataFrame)(polars_internal_1.default.scanParquet(path, pliOptions));
}
exports.scanParquet = scanParquet;
function readIPC(pathOrBody, options = {}) {
    if (Buffer.isBuffer(pathOrBody)) {
        return (0, dataframe_1._DataFrame)(polars_internal_1.default.readIpc(pathOrBody, options));
    }
    if (typeof pathOrBody === "string") {
        const inline = !(0, utils_1.isPath)(pathOrBody, [".ipc"]);
        if (inline) {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readIpc(Buffer.from(pathOrBody, "utf-8"), options));
        }
        else {
            return (0, dataframe_1._DataFrame)(polars_internal_1.default.readIpc(pathOrBody, options));
        }
    }
    else {
        throw new Error("must supply either a path or body");
    }
}
exports.readIPC = readIPC;
function scanIPC(path, options = {}) {
    return (0, dataframe_2._LazyDataFrame)(polars_internal_1.default.scanIpc(path, options));
}
exports.scanIPC = scanIPC;
function readCSVStream(stream, options) {
    const batchSize = options?.batchSize ?? 10000;
    let count = 0;
    const end = options?.endRows ?? Number.POSITIVE_INFINITY;
    return new Promise((resolve, reject) => {
        const s = stream.pipe(new LineBatcher({ batchSize }));
        const chunks = [];
        s.on("data", (chunk) => {
            // early abort if 'end rows' is specified
            if (count <= end) {
                chunks.push(chunk);
            }
            else {
                s.end();
            }
            count += batchSize;
        }).on("end", () => {
            try {
                const buff = Buffer.concat(chunks);
                const df = readCSVBuffer(buff, options);
                resolve(df);
            }
            catch (err) {
                reject(err);
            }
        });
    });
}
exports.readCSVStream = readCSVStream;
function readJSONStream(stream, options = readJsonDefaultOptions) {
    options = { ...readJsonDefaultOptions, ...options };
    return new Promise((resolve, reject) => {
        const chunks = [];
        stream
            .pipe(new LineBatcher({ batchSize: options.batchSize }))
            .on("data", (chunk) => {
            try {
                const df = (0, dataframe_1._DataFrame)(polars_internal_1.default.readJson(chunk, options));
                chunks.push(df);
            }
            catch (err) {
                reject(err);
            }
        })
            .on("end", () => {
            try {
                const df = (0, functions_1.concat)(chunks);
                resolve(df);
            }
            catch (err) {
                reject(err);
            }
        });
    });
}
exports.readJSONStream = readJSONStream;
